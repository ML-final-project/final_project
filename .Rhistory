(test_mse <- augment(nes_lm, newdata = nes_test) %>%
mse(truth = biden, estimate = .fitted))
#Rei: replicate function makes doing it much easier than like looping
#source https://www.datamentor.io/r-programming/repeat-loop/
x <- 1
mse_list <- c()
repeat{
nes_split <- initial_split(data = nes2008_df,
prop = 0.5) #split the data in half
nes_train <- training(nes_split) #random subsample to train
nes_test <- testing(nes_split)
#Fit the linear regression model using only the training observations.
nes_lm <- glm(biden~female+age+educ+dem+rep, data = nes_train) #fit model on training   data
mse <- augment(nes_lm, newdata = nes_test) %>%
mse(truth = biden, estimate = .fitted)%>%
select(.estimate)%>%
as.numeric()
mse_list <- append(mse_list,mse)
x= x+1
if (x == 1000){
break
}
}
mean(mse_list)
data <- data.frame(mse = mse_list)
ggplot(data, aes(x = mse))+
geom_histogram(binwidth = 1, alpha = 0.75)
regn_model <- glm(biden ~ female + age + educ + dem + rep, data = nes2008_df)
#summary(regn_model)$coefficients
analysis(regn_model)
# bootstrapped estimates of the parameter estimates and standard errors
lm_coefs <- function(nes2008_df, ...) {
## use `analysis` or `as.data.frame` to get the analysis data
mod <- lm(..., data = analysis(nes2008_df))
summary(mod)$coefficients
#analysisi(mod)
}
biden_boot <- nes2008_df %>%
bootstraps(1000) %>%
mutate(coef = map(splits, lm_coefs, as.formula(biden ~ female + age + educ + dem + rep)))
biden_boot %>%
unnest(coef) %>%
group_by(term) %>%
summarize(.estimate = mean(estimate),
.se = sd(estimate, na.rm = TRUE))
#analysisi(mod)
tidy(mod)
tidy(mod)
library(ISLR)
tidy(mod)
tidy(auto_lm) #another way to show regn output
#analysisi(mod)
augment(mod)
regn_model <- lm(biden ~ female + age + educ + dem + rep, data = nes2008_df)
tidy(regn_model)
library(tidyverse)
library(ISLR) #to load the data
library(broom)
library(rsample)
library(rcfss)
library(yardstick)
#devtools:: install_github("uc-cfss/rcfss") #how install for stuff not on cran
Auto <- as_tibble(Auto)
set.seed(1234)
auto_split <- initial_split(data = Auto,
prop = 0.5) #split the data in half
auto_train <- training(auto_split) #random subsample to train
auto_test <- testing(auto_split)
library(ISLR)
Auto <- as_tibble(Auto)
skimr::skim(Auto) #nicer regn output
library(ISLR)
Auto <- as_tibble(Auto)
#skimr::skim(Auto) #nicer regn output
# plot the data and model
ggplot(Auto, aes(horsepower, mpg)) +
geom_point() +
geom_smooth(method = "lm") #put linear fit line over it! can do lowess instead
# traditional parameter estimates and standard errors
auto_lm <- lm(mpg ~ poly(horsepower, 1, raw = TRUE), data = Auto)
tidy(auto_lm) #another way to show regn output
regn_model <- lm(biden ~ female + age + educ + dem + rep, data = nes2008_df)
summary(regn_model)
# bootstrapped estimates of the parameter estimates and standard errors
lm_coefs <- function(nes2008_df, ...) {
## use `analysis` or `as.data.frame` to get the analysis data
mod <- lm(..., data = analysis(nes2008_df))
#tidy(mod)
summary(mod)
}
biden_boot <- nes2008_df %>%
bootstraps(1000) %>%
mutate(coef = map(splits, lm_coefs, as.formula(biden ~ female + age + educ + dem + rep)))
biden_boot$coef[2]
unnest(coef)
biden_boot %>%
unnest(coef) %>%
group_by(term) %>%
summarize(.estimate = mean(estimate),
.se = sd(estimate, na.rm = TRUE))
biden_boot %>%
#unnest(coef) %>%
#group_by(term) %>%
summarize(.estimate = mean(estimate),
.se = sd(estimate, na.rm = TRUE))
View(biden_boot)
biden_boot$coef[2]
summarise(biden_boot$coef)
mean(biden_boot$coef)
biden_boot$coef
unnest(biden_boot$coef)
biden_boot$coef[[1][1]]
#tidy(mod)
summarize(mod)
tidy(mod)
tidy(regn_model)
broom::tidy()
tidy(regn_model)
# bootstrapped estimates of the parameter estimates and standard errors
lm_coefs <- function(splits, ...) {
## use `analysis` or `as.data.frame` to get the analysis data
mod <- lm(..., data = analysis(splits))
#tidy(mod)
summary(mod)
}
biden_boot <- nes2008_df %>%
bootstraps(1000) %>%
mutate(coef = map(splits, lm_coefs, as.formula(biden ~ female + age + educ + dem + rep)))
biden_boot %>%
unnest(coef) %>%
group_by(term) %>%
summarize(.estimate = mean(estimate),
.se = sd(estimate, na.rm = TRUE))
remove.packages("broom")
install.packages("broom")
tibble(c("state", "temp" "test"),
c("idk", "what", "eh")
)
tibble(c("state", "temp" "test"),
c("idk", "what", "eh"))
tibble(c("state", "temp" "test"),
c("idk", "what", "eh"))
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962)
tibble("state" = c("AK", "IL", "CO", "FL", "CA"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962))
tibble::tibble("state" = c("AK", "IL", "CO", "FL", "CA"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("precipitation",
"precipitation",
"precipitation",
"precipitation",
"precipitation",)
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("precipitation",
"precipitation",
"precipitation",
"precipitation",
"precipitation")
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("precipitation",
"precipitation",
"precipitation",
"precipitation",
"precipitation"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation"),
)
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"State" = c("AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Snow",
"Snow",
"Snow",
"Snow",
"Snow"))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA",
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA",
"AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Snow",
"Snow",
"Snow",
"Snow",
"Snow"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962,
1.0, 0.976, 1.0, NaN, NaN),
"Mean" = c(1.0, 0.991, 1.0, 0.994, 0.991,
1.0, 0.994, 1.0, nan, nan))
<-
jdlfjdlk
)
)))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA",
"AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Snow",
"Snow",
"Snow",
"Snow",
"Snow"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962,
1.0, 0.976, 1.0, NaN, NaN),
"Mean" = c(1.0, 0.991, 1.0, 0.994, 0.991,
1.0, 0.994, 1.0, nan, nan))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA",
"AK", "IL", "CO", "FL", "CA"),
"Variable" = c("Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Precipitation",
"Snow",
"Snow",
"Snow",
"Snow",
"Snow"),
"Min" = c(1.0, 0.972, 1.0, 0.986, 0.962,
1.0, 0.976, 1.0, NaN, NaN),
"Mean" = c(1.0, 0.991, 1.0, 0.994, 0.991,
1.0, 0.994, 1.0, NaN, NaN))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN))
tibble::tibble("State" = c("AK", "IL", "CO", "FL", "CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN),
"Min_tavg" = c(1.0, 0.999, 1.0, 1.0, 0.975),
"Mean_tavg" = c(1.0, 1.0, 1.0, 1.0, 0.995))
tibble::tibble("State" = c("Anchorage, AK", "Cook County, IL",
"Denver County, CO", "Hillsborough, FL",
"Los Angeles, CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN),
"Min_tavg" = c(1.0, 0.999, 1.0, 1.0, 0.975),
"Mean_tavg" = c(1.0, 1.0, 1.0, 1.0, 0.995))
noaa_corr <- tibble::tibble("State" = c("Anchorage, AK", "Cook County, IL",
"Denver County, CO", "Hillsborough, FL",
"Los Angeles, CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN),
"Min_tavg" = c(1.0, 0.999, 1.0, 1.0, 0.975),
"Mean_tavg" = c(1.0, 1.0, 1.0, 1.0, 0.995))
stargazer::stargazer(noaa_corr)
noaa_corr <- tibble::tibble("State" = c("Anchorage, AK", "Cook County, IL",
"Denver County, CO", "Hillsborough, FL",
"Los Angeles, CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN),
"Min_tavg" = c(1.0, 0.999, 1.0, 1.0, 0.975),
"Mean_tavg" = c(1.0, 1.0, 1.0, 1.0, 0.995))
stargazer::stargazer(noaa_corr, summary = FALSE, title = "Within county NOAA correlations")
library(tidyverse)
read_csv("C:\Users\User02\Desktop\data table.csv")
read_csv("C:/Users/User02/Desktop/data table.csv")
read_csv("C:/Users/User02/Desktop/data_table.csv")
read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
temp <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
View(temp)
temp <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
View(temp)
temp <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
View(temp)
vars <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv")
vars <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv",
index = FALSE)
vars <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv") %>%
select(-X1)
View(vars)
stargazer::stargazer(vars, summary = FALSE, title = "List of data and study information")
vars <- read_csv("C:/Users/User02/Desktop/tables/data_table.csv") %>%
select(-X1)
noaa_corr <- tibble::tibble("State" = c("Anchorage, AK", "Cook County, IL",
"Denver County, CO", "Hillsborough, FL",
"Los Angeles, CA"),
"Min_prcp" = c(1.0, 0.972, 1.0, 0.986, 0.962),
"Mean_prcp" = c(1.0, 0.991, 1.0, 0.994, 0.991),
"Min_snow" = c(1.0, 0.976, 1.0, NaN, NaN),
"Mean_snow" = c(1.0, 0.994, 1.0, NaN, NaN),
"Min_tavg" = c(1.0, 0.999, 1.0, 1.0, 0.975),
"Mean_tavg" = c(1.0, 1.0, 1.0, 1.0, 0.995))
stargazer::stargazer(noaa_corr, summary = FALSE, title = "Within county NOAA correlations")
rm(list = la())
rm(list = ls())
library(yaml)
library(rprojroot)
library(tidyverse)
library(lubridate)
library(snakecase)
library(sf)
setwd("~/final_project/")
make_path <- is_git_root$make_fix_file()
config <- yaml.load_file(make_path("analysis/config.yml"))
out_path <- make_path(config$build_path)
data_out <- make_path(config$data_path$merge)
source(str_c(config$group_code, "prelim.R"))
noaa <- read_csv(make_path(config$data_path$noaa,
"Mass2010Normal.csv")) %>%
set_names(to_snake_case(colnames(.))) %>%
select(station_name,
date,
elevation,
latitude,
longitude,
contains("normal")) %>%
na_if(-9999) %>%
na.omit() %>%
mutate(date = ymd(date)) %>%
mutate(state = case_when(str_detect(station_name, "CA") ~ "CA",
str_detect(station_name, "IL") ~ "IL",
str_detect(station_name, "AK") ~ "AK",
str_detect(station_name, "CO") ~ "CO",
str_detect(station_name, "FL") ~ "FL")) %>%
st_as_sf(coords = c("longitude", "latitude"),
crs = 4326, remove = FALSE) %>%
st_transform("+proj=utm +zone=42N +datum=WGS84 +units=km")
pm25_read <- tibble()
for (file in grep("?.csv",
list.files(make_path(config$data_path$pm25),
pattern = "*2010.csv"),
value = T)) {
.tmp <- read_csv(make_path(config$data_path$pm25, file))
pm25_read <- pm25_read %>% rbind(.tmp)
}
pm25 <- pm25_read %>%
set_names(to_snake_case(colnames(.))) %>%
mutate(date = mdy(date)) %>%
st_as_sf(coords = c("site_longitude", "site_latitude"),
crs = 4326, remove = FALSE) %>%
st_transform("+proj=utm +zone=42N +datum=WGS84 +units=km") %>%
na.omit() %>%
mutate(state = case_when(state == "California" ~ "CA",
state == "Illinois" ~ "IL",
state == "Alaska" ~ "AK",
state == "Colorado" ~ "CO",
state == "Florida" ~ "FL")) %>%
select(date, site_name, daily_mean_pm_2_5_concentration, state)
View(pm25)
View(noaa)
CA <- noaa %>% filter(state == "CA")
View(CA)
unique(CA$latitude)
unique(CA$elevation)
IL <- noaa %>% filter(state == "IL")
unique(IL$elevation)
CO <- noaa %>% filter(state == "CO")
unique(CO$elevation)
FL <- noaa %>% filter(state == "FL")
unique(FL$elevation)
AK <- noaa %>% filter(state == "AK")
unique(AK$elevation)
IL %>% min(mtd_prcp_normal)
View(IL)
min(IL$mtd_prcp_normal)
max(IL$mtd_prcp_normal)
min(CA$mtd_prcp_normal)
max(CA$mtd_prcp_normal)
min(CO$mtd_prcp_normal)
max(CO$mtd_prcp_normal)
min(FL$mtd_prcp_normal)
max(FL$mtd_prcp_normal)
min(AK$mtd_prcp_normal)
max(AK$mtd_prcp_normal)
min(IL$dly_tavg_normal)
max(IL$dly_tavg_normal)
min(IL$dly_tmax_normal)
min(IL$dly_tmin_normal)
min(CA$dly_tavg_normal)
max(CA$dly_tavg_normal)
min(CO$dly_tavg_normal)
max(CO$dly_tavg_normal)
max(FL$dly_tavg_normal)
min(FL$dly_tavg_normal)
min(AK$dly_tavg_normal)
max(AK$dly_tavg_normal)
unique(AK$station_name)
unique(FL$station_name)
unique(CO$station_name)
unique(CA$station_name)
unique(IL$station_name)
CA <- pm25 %>% filter(state = "CA")
CA <- pm25 %>% filter(state == "CA")
IL <- pm25 %>% filter(state == "IL")
CO <- pm25 %>% filter(state == "CO")
AK <- pm25 %>% filter(state == "AK")
FL <- pm25 %>% filter(state == "FL")
unique(IL$site_name)
unique(CA$site_name)
unique(CO$site_name)
unique(FL$site_name)
unique(AK$site_name)
View(pm25)
View(AK)
View(CA)
unique(IL$site_name)
unique(AK$site_name)
unique(FL$site_name)
unique(CO$site_name)
unique(CA$site_name)
unique(noaa$station_name)
unique(pm25$site_name)
CA <- noaa %>% filter(state == "CA")
IL <- noaa %>% filter(state == "IL")
FL <- noaa %>% filter(state == "FL")
CO <- noaa %>% filter(state == "CO")
AK <- noaa %>% filter(state == "AK")
mean(CA$elevation)
mean(IL$elevation)
mean(CO$elevation)
mean(FL$elevation)
mean(AK$elevation)
mean(IL$mtd_prcp_normal)
mean(CA$mtd_prcp_normal)
mean(CO$mtd_prcp_normal)
mean(FL$mtd_prcp_normal)
mean(AK$mtd_prcp_normal)
mean(IL$dly_tavg_normal)
mean(CA$dly_tavg_normal)
mean(CO$dly_tavg_normal)
mean(FL$dly_tavg_normal)
mean(AK$dly_tavg_normal)
pm25_buf <- pm25 %>%
st_buffer(15)
noaa <- read_csv(make_path(config$data_path$noaa,
"Mass2010Normal.csv")) %>%
set_names(to_snake_case(colnames(.))) %>%
select(station_name,
date,
elevation,
latitude,
longitude,
contains("normal")) %>%
na_if(-9999) %>%
na.omit() %>%
mutate(date = ymd(date)) %>%
mutate(state = case_when(str_detect(station_name, "CA") ~ "CA",
str_detect(station_name, "IL") ~ "IL",
str_detect(station_name, "AK") ~ "AK",
str_detect(station_name, "CO") ~ "CO",
str_detect(station_name, "FL") ~ "FL")) %>%
st_as_sf(coords = c("longitude", "latitude"),
crs = 4326, remove = FALSE) %>%
st_transform("+proj=utm +zone=42N +datum=WGS84 +units=km")
pm25_read <- tibble()
for (file in grep("?.csv",
list.files(make_path(config$data_path$pm25),
pattern = "*2010.csv"),
value = T)) {
.tmp <- read_csv(make_path(config$data_path$pm25, file))
pm25_read <- pm25_read %>% rbind(.tmp)
}
pm25 <- pm25_read %>%
set_names(to_snake_case(colnames(.))) %>%
mutate(date = mdy(date)) %>%
st_as_sf(coords = c("site_longitude", "site_latitude"),
crs = 4326, remove = FALSE) %>%
st_transform("+proj=utm +zone=42N +datum=WGS84 +units=km") %>%
na.omit() %>%
mutate(state = case_when(state == "California" ~ "CA",
state == "Illinois" ~ "IL",
state == "Alaska" ~ "AK",
state == "Colorado" ~ "CO",
state == "Florida" ~ "FL")) %>%
select(date, site_name, daily_mean_pm_2_5_concentration, state)
pm25_buf <- pm25 %>%
st_buffer(15)
View(pm25_buf)
intersections <- st_intersection(pm25_buf, noaa)
View(noaa)
intersections <- st_intersection(pm25_buf, noaa)
View(intersections)
